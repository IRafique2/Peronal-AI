{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Instant-Response Chatbot with Sub-Second Replies\"\"\"\n",
        "\n",
        "!pip install gradio transformers --quiet\n",
        "!pip install google-generativeai --quiet\n",
        "\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "import time\n",
        "from typing import List, Tuple\n",
        "\n",
        "# 1. LIGHTWEIGHT CONFIGURATION\n",
        "# ============================\n",
        "GOOGLE_API_KEY = os.getenv('GEMINI_KEY')  # Set this in your environment\n",
        "MAX_RESPONSE_TOKENS = 120  # Shorter responses\n",
        "TIMEOUT_SECONDS = 3  # Fail fast if response takes too long\n",
        "CACHE_SIZE = 10  # Number of responses to cache\n",
        "\n",
        "# 2. OPTIMIZED MODEL LOADING\n",
        "# ==========================\n",
        "print(\"âš¡ Loading optimized models...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Use smaller, faster emotion model\n",
        "emotion_pipe = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"finiteautomata/bertweet-base-sentiment-analysis\",\n",
        "    device=-1  # Force CPU for faster cold starts\n",
        ")\n",
        "\n",
        "# Configure Gemini for speed\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "generation_config = {\n",
        "    \"temperature\": 0.8,\n",
        "    \"top_p\": 1.0,\n",
        "    \"top_k\": 1,  # Fewer options = faster\n",
        "    \"max_output_tokens\": MAX_RESPONSE_TOKENS\n",
        "}\n",
        "model = genai.GenerativeModel(\n",
        "    'gemini-pro',\n",
        "    generation_config=generation_config\n",
        ")\n",
        "\n",
        "print(f\"âœ… Models loaded in {time.time()-start_time:.2f}s\")\n",
        "\n",
        "# 3. RESPONSE CACHING SYSTEM\n",
        "# ==========================\n",
        "response_cache = {}\n",
        "\n",
        "def get_cached_response(prompt: str) -> str:\n",
        "    \"\"\"Check cache before generating new response\"\"\"\n",
        "    if prompt in response_cache:\n",
        "        return response_cache[prompt]\n",
        "    return None\n",
        "\n",
        "# 4. ULTRA-FAST RESPONSE GENERATION\n",
        "# =================================\n",
        "def generate_instant_response(user_input: str) -> str:\n",
        "    \"\"\"Optimized pipeline with multiple fallbacks\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Check cache first\n",
        "    cached = get_cached_response(user_input[:100])  # First 100 chars as key\n",
        "    if cached:\n",
        "        return cached\n",
        "\n",
        "    # Fast path - simple response patterns\n",
        "    lower_input = user_input.lower()\n",
        "    if any(greeting in lower_input for greeting in [\"hi\", \"hello\", \"hey\"]):\n",
        "        return \"Hello! How can I help you today?\"\n",
        "    if \"thank\" in lower_input:\n",
        "        return \"You're welcome! Is there anything else?\"\n",
        "\n",
        "    # Medium path - try quick generation\n",
        "    try:\n",
        "        prompt = f\"Brief response to: {user_input[:150]}\"  # Truncate input\n",
        "        response = model.generate_content(\n",
        "            prompt,\n",
        "            request_options={\"timeout\": TIMEOUT_SECONDS}\n",
        "        )\n",
        "        if response.text:\n",
        "            # Cache successful responses\n",
        "            response_cache[user_input[:100]] = response.text[:MAX_RESPONSE_TOKENS]\n",
        "            if len(response_cache) > CACHE_SIZE:\n",
        "                response_cache.pop(next(iter(response_cache)))\n",
        "            return response.text[:MAX_RESPONSE_TOKENS]\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Fallback path\n",
        "    fallbacks = [\n",
        "        \"I appreciate your message. Could you tell me more?\",\n",
        "        \"That's interesting. What else is on your mind?\",\n",
        "        \"Thanks for sharing. How can I assist you further?\"\n",
        "    ]\n",
        "    return fallbacks[len(user_input) % len(fallbacks)]\n",
        "\n",
        "# 5. STREAMLINED CHAT INTERFACE\n",
        "# =============================\n",
        "def chat_fn(message: str, history: List[Tuple[str, str]]):\n",
        "    \"\"\"Ultra-fast chat function\"\"\"\n",
        "    start_time = time.time()\n",
        "    response = generate_instant_response(message)\n",
        "    history.append((message, response))\n",
        "    print(f\"Generated response in {time.time()-start_time:.3f}s\")\n",
        "    return \"\", history\n",
        "\n",
        "# 6. MINIMAL GRADIO UI\n",
        "# ====================\n",
        "with gr.Blocks() as app:\n",
        "    gr.Markdown(\"## âš¡ Instant Chat\")\n",
        "\n",
        "    chatbot = gr.Chatbot(\n",
        "        height=300,\n",
        "        layout=\"compact\",\n",
        "        show_label=False\n",
        "    )\n",
        "\n",
        "    msg = gr.Textbox(\n",
        "        placeholder=\"Type your message...\",\n",
        "        max_lines=2,\n",
        "        container=False\n",
        "    )\n",
        "\n",
        "    clear_btn = gr.Button(\"Clear\", size=\"sm\")\n",
        "\n",
        "    msg.submit(\n",
        "        chat_fn,\n",
        "        [msg, chatbot],\n",
        "        [msg, chatbot]\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        lambda: None,\n",
        "        None,\n",
        "        chatbot,\n",
        "        queue=False\n",
        "    )\n",
        "\n",
        "# 7. PERFORMANCE-OPTIMIZED LAUNCH\n",
        "# ===============================\n",
        "print(\"\\nðŸš€ Launching ultra-fast chat interface...\")\n",
        "app.launch(\n",
        "    share=True,\n",
        "    max_threads=1,  # Single thread for stability\n",
        "    # Removed enable_queue=True as it's not a valid argument\n",
        "    show_error=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "id": "7eX3y7jfAh4v",
        "outputId": "5051ccff-aa99-479c-e291-11f330404f48"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš¡ Loading optimized models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
            "Device set to use cpu\n",
            "/tmp/ipython-input-19-1972491569.py:115: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Models loaded in 0.77s\n",
            "\n",
            "ðŸš€ Launching ultra-fast chat interface...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ac86f60e39dcc6ca56.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ac86f60e39dcc6ca56.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}